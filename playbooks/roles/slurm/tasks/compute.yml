---
- name: Run Pam settings
  include_tasks: compute_pam.yml
  when: pam|bool

- name: install SLURM compute packages
  vars: 
    package_name: '{{ slurm_compute_packages }}'
    package_repo: "{{ slurm_repos }}"
    disable_gpg_check_var: True
  include_role: 
    name: safe_yum

- name: Render systemd units for slurmd
  become: true
  template:
    src: 'systemd/{{ item }}.service.j2'
    dest: '/lib/systemd/system/{{ item }}.service'
    backup: "yes"
  with_items:
    - slurmd
  when: ansible_os_family == 'Debian'

- name: Create systemd unit dirs
  become: true
  file:
    name: '/etc/systemd/system/{{ item }}.service.d'
    state: directory
  with_items:
    - munge
    - slurmd

- name: Render systemd units for slurmd and munge
  become: true
  template:
    src: 'systemd/{{ item }}.service.d/unit.conf.j2'
    dest: '/etc/systemd/system/{{ item }}.service.d/unit.conf'
    backup: "yes"
  with_items:
    - munge
    - slurmd

- name: Create munge dir
  become: true
  file:
    name: '{{ munge_conf_path }}'
    state: directory
    owner: munge
    group: munge
    mode: 0700

- name: copy munge.key to tmp
  become: true
  shell:
    cmd: cp /etc/munge/munge.key /tmp/munge.key
  delegate_to: 127.0.0.1
  run_once: true

- name: set permissions
  become: true
  shell:
    cmd: chown {{ controller_username }}:{{ controller_username }} /tmp/munge.key
  delegate_to: 127.0.0.1
  run_once: true

- name: Copy munge key
  become: true
  copy:
    src: /tmp/munge.key
    dest: /etc/munge/munge.key
    owner: munge
    group: munge
    mode: '0400'
  notify: restart munge

- name: restart munge
  become: true
  service:
    name: munge
    state: restarted
    enabled: true

- name: Add to the gres.conf file on the controller.
  become: true
  blockinfile:
    block: "{{ lookup('template', 'gres.conf.j2') }}"
    path: "{{ slurm_conf_path }}/gres.conf"
    marker: "### {mark} {{ansible_hostname}}"
  throttle: 1
  delegate_to: 127.0.0.1
  when: "'GPU' in shape"

- name: Get hostnames
  set_fact:
    nodes_to_add_temp: "{{hostvars[item]['ansible_hostname']}}"
  with_items: "{{ play_hosts | difference(groups['controller']) | difference(groups['slurm_backup']) | difference(groups['login']) | difference(groups['monitoring']) }}"
  run_once: true
  register: nodes_to_add_temp_results

- name: Make a list of nodes to add
  set_fact: nodes_to_add="{{nodes_to_add_temp_results.results | map(attribute='ansible_facts.nodes_to_add_temp') | list }}"
  run_once: true

- name: Get nodes from Cluster Switch
  block:
    - name: Get nodes from topology.conf
      shell: "cat {{ slurm_conf_path }}/topology.conf | grep \"SwitchName={{cluster_name}}\""
      register: cluster_switch
    - name: Get cluster_hostlist
      command: "scontrol show hostname {{cluster_switch.stdout.split('Nodes=')[1]}}"
      register: cluster_hostlist
    - name: Create new cluster_hostlist
      command: "scontrol show hostlistsorted {{cluster_hostlist.stdout_lines | union(nodes_to_add) | join(',') }}"
      register: cluster_hostlist_condensed_results
  rescue:
    - name: Create existing cluster list
      command: "scontrol show hostlistsorted {{ nodes_to_add | join(',') }}"
      register: cluster_hostlist_condensed_results
  delegate_to: 127.0.0.1
  #run_once: true # There is a bug in Ansible that is causing nodes to fail with the block and run_once combo. Uncomment when fixed. 

- name: add nodes to Switch
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{cluster_name}}\\sNodes.*"
    line: "SwitchName={{cluster_name}} Nodes={{ cluster_hostlist_condensed_results.stdout }}"
    state: present
  run_once: true
  delegate_to: 127.0.0.1
  notify: reconfigure slurm

- name: Run Pam settings
  include_tasks: compute_pam.yml
  when: pam|bool

- name: start slurmd
  become: true
  service:
    name: slurmd
    state: restarted
    enabled: true
    
- name: Reconfigure Slurm for topology
  become: true
  command: "scontrol reconfigure"
  delegate_to: 127.0.0.1
  run_once: true