---

- name: install SLURM compute packages
  vars:
    package_name: '{{ slurm_compute_packages }}'
    package_repo: "{{ slurm_repos }}"
    disable_gpg_check_var: True
  include_role:
    name: safe_yum

- name: Render systemd units for slurm, slurmdbd and munge
  become: true
  template:
    src: 'systemd/{{ item }}.service.j2'
    dest: '/lib/systemd/system/{{ item }}.service'
    backup: "yes"
  with_items:
    - slurmd
  when: ansible_os_family == 'Debian'

- name: Create systemd unit dirs
  become: true
  file:
    name: '/etc/systemd/system/{{ item }}.service.d'
    state: directory
  with_items:
    - munge
    - slurmd

- name: Render systemd units for slurmd and munge
  become: true
  template:
    src: 'systemd/{{ item }}.service.d/unit.conf.j2'
    dest: '/etc/systemd/system/{{ item }}.service.d/unit.conf'
    backup: "yes"
  with_items:
    - munge
    - slurmd

- name: Create munge dir
  become: true
  file:
    name: '{{ munge_conf_path }}'
    state: directory
    owner: munge
    group: munge
    mode: 0700

- name: copy munge.key to tmp
  become: true
  shell:
    cmd: cp /etc/munge/munge.key /tmp/munge.key
  delegate_to: 127.0.0.1
  run_once: true

- name: set permissions
  become: true
  shell:
    cmd: chown {{ controller_username }}:{{ controller_username }} /tmp/munge.key
  delegate_to: 127.0.0.1
  run_once: true

- name: Copy munge key
  become: true
  copy:
    src: /tmp/munge.key
    dest: /etc/munge/munge.key
    owner: munge
    group: munge
    mode: '0400'
  notify: restart munge

- name: restart munge
  become: true
  service:
    name: munge
    state: restarted
    enabled: true

- name: Add to the gres.conf file on the controller.
  become: true
  blockinfile:
    block: "{{ lookup('template', 'gres.conf.j2') }}"
    path: "{{ slurm_conf_path }}/gres.conf"
    marker: "### {mark} {{ansible_hostname}}"
  throttle: 1
  delegate_to: 127.0.0.1
  when: "'GPU' in shape"

- name: Run the script to get the RackID
  shell: 'curl -sH "Authorization: Bearer Oracle" -L http://169.254.169.254/opc/v1/host | jq .rackId'
#  shell: echo $RANDOM | md5sum | head -c 20
  register: rackID_script
  retries: 5
  delay: 5
  until: rackID_script is not failed

- name: H100/H200 Block
  block: 
    - name: Run the script to get the RDMA Block ID
      shell: 'curl -sH "Authorization: Bearer Oracle" -L http://169.254.169.254/opc/v1/host/rdmaTopologyData/customerLocalBlock | grep ocid'
    #  shell: echo $RANDOM | md5sum | head -c 20
      register: blockID_script
      retries: 3
      delay: 1
      ignore_errors: true
      until: blockID_script is not failed

    - name: Set BlockID fact
      set_fact:
        rackID: "{{ blockID_script.stdout.split('.')[4][-16:-1]}}"
      when: blockID_script is not failed 

    - name: Set RackID fact
      set_fact:
        rackID: "{{ rackID_script.stdout[1:-41]}}"
      when: blockID_script is failed
  when: shape == 'BM.GPU.H100.8' or shape == 'BM.GPU.H200.8'

- name: Set Rack ID fact on nodes other than H100
  set_fact:
    rackID: "{{ rackID_script.stdout[1:-41]}}"
  when: shape != 'BM.GPU.H100.8' and shape != 'BM.GPU.H200.8'

- name: Get rackIDs for all compute nodes
  set_fact:
    racks_to_add_temp: "{{cluster_name}}:{{hostvars[item]['rackID']}}"
  with_items: "{{ play_hosts | difference(groups['controller'])  | difference(groups['slurm_backup']) | difference(groups['login'])| difference(groups['monitoring'])}}"
  run_once: true
  register: racks_to_add_temp_results

- name: Make a list of racks to add
  set_fact: racks_to_add="{{racks_to_add_temp_results.results | map(attribute='ansible_facts.racks_to_add_temp') | list | unique}}"
  run_once: true

- name: Get hostnames
  set_fact:
    nodes_to_add_temp: "{{hostvars[item]['ansible_hostname']}}"
  with_items: "{{ play_hosts | difference(groups['controller']) | difference(groups['slurm_backup']) | difference(groups['login']) | difference(groups['monitoring'])}}"
  run_once: true
  register: nodes_to_add_temp_results

- name: Make a list of nodes to add
  set_fact: nodes_to_add="{{nodes_to_add_temp_results.results | map(attribute='ansible_facts.nodes_to_add_temp') | list}}"
  run_once: true

- name: Get current nodes in Switch
  shell: "cat {{ slurm_conf_path }}/topology.conf | grep \"{{item}}\" | grep Nodes="
  register: nodes_in_switch
  delegate_to: 127.0.0.1
  run_once: true
  with_items: "{{racks_to_add}}"
  ignore_errors: yes

- name: Get current nodes in Switch hostlist
  vars:
    - switch_list_condensed: "{{item.stdout.split('Nodes=')[1]}}"
  command: "scontrol show hostname {{switch_list_condensed }}"
  register: switch_hostlist
  delegate_to: 127.0.0.1
  with_items: "{{nodes_in_switch.results}}"
  when: item.rc == 0

- name: Get hostlist if switch exists
  vars:
    new_line: "{% for node in ( play_hosts | difference(groups['controller']) | difference(groups['slurm_backup'])  | difference(groups['login']) | difference(groups['monitoring'])) %}{% if cluster_name+':'+hostvars[node]['rackID'] == item.item.item %}{{hostvars[node]['ansible_hostname']}},{% endif %}{% endfor %}"
  command: "scontrol show hostlistsorted {{ item.stdout_lines | union (new_line[:-1].split(',') | list )| join(',') }}"
  register: rack_hostlist1
  delegate_to: 127.0.0.1
  with_items: "{{switch_hostlist.results}}"
  run_once: true
  when: item.item.rc == 0

- name: Get hostlist if switch does not exists
  vars:
    new_line: "{% for node in ( play_hosts | difference(groups['controller'])  | difference(groups['slurm_backup']) | difference(groups['login']) | difference(groups['monitoring'])) %}{% if cluster_name+':'+hostvars[node]['rackID'] == item.item.item %}{{hostvars[node]['ansible_hostname']}},{% endif %}{% endfor %}"
  command: "scontrol show hostlistsorted {{ new_line[:-1] }}"
  register: rack_hostlist2
  delegate_to: 127.0.0.1
  with_items: "{{switch_hostlist.results}}"
  run_once: true
  when: item.item.rc > 0

- name: get Nodes on switch
  set_fact:
    nodes_on_switches: "{{nodes_on_switches | default({}) | combine({item.item.item.item : item.stdout } ) }}"
  with_items: "{{rack_hostlist1.results}}"
  run_once: true
  delegate_to: 127.0.0.1
  when: item.item.item.rc== 0

- name: get Nodes on switch
  set_fact:
    nodes_on_switches: "{{nodes_on_switches | default({}) | combine({item.item.item.item : item.stdout } ) }}"
  with_items: "{{rack_hostlist2.results}}"
  run_once: true
  delegate_to: 127.0.0.1
  when: item.item.item.rc > 0

- name: Add the nodes in the rack switches
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{item.item.item}}\\sNodes.*"
    line: "SwitchName={{item.item.item}} Nodes={{nodes_on_switches[item.item.item]}}"
    state: present
  run_once: true
  with_items: "{{switch_hostlist.results}}"
  delegate_to: 127.0.0.1
  notify: reconfigure slurm

- name: Get racks in the Cluster
  block:
    - name: Get Racks from topology.conf
      shell: "cat {{ slurm_conf_path }}/topology.conf | grep \"SwitchName={{cluster_name}}:\" | awk '{print $1}' | sed 's/SwitchName=//' | tr '\n' ',' | sed 's/,$/\\n/'"
      register: racks_left
      run_once: true
      delegate_to: 127.0.0.1
    - name: Create list of racks
      set_fact:
        racks_left_list: "{{racks_left.stdout.split(',') | list}}"
  rescue:
    - name: Empty racks
      set_fact:
        racks_left_list: []

- name: Update Rack switches for the cluster
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{cluster_name}}\\sSwitches.*"
    line: "SwitchName={{cluster_name}} Switches={{ racks_left_list | join(',') }}"
    state: present
  run_once: true
  delegate_to: 127.0.0.1
  notify: reconfigure slurm
  when: racks_left_list | length > 0

- name: Run Pam settings
  include_tasks: compute_pam.yml
  when: pam|bool

- name: run handlers
  meta: flush_handlers
  
- name: start slurmd
  become: true
  service:
    name: slurmd
    state: restarted
    enabled: true

- name: Give some time to the slurmd to start
  pause:
    seconds: 10

- name: Reconfigure Slurm for topology
  become: true
  command: "scontrol reconfigure"
  delegate_to: 127.0.0.1
  run_once: true