---
- name: Generate password for DB user and save it to /etc/trinity/passwords
  set_fact:
    tmp_pwd: '{{ lookup("password",
                           "/etc/opt/oci-hpc/passwords/mysql/slurmdbd.txt
                            chars=ascii_letters,digits,hexdigits") }}'

- name: Get password for DB user from /etc/trinity/passwords)
  set_fact:
    slurmdbd_sql_pwd: '{{ lookup("password",
                           "/etc/opt/oci-hpc/passwords/mysql/slurmdbd.txt
                            chars=ascii_letters,digits,hexdigits") }}'

- name: Install PyMySQL
  vars: 
    package_name:
      - python2-PyMySQL
    package_repo: "{{slurm_repos}}"
    package_state: present
  include_role: 
    name: safe_yum
  when: ansible_os_family == 'RedHat'

- name: Install libmariadb-dev
  vars: 
    package_name:
      - libmariadb-dev
    package_state: present
  include_role: 
    name: safe_yum
  when: ansible_os_family == 'Debian'

- name: install SLURM server packages RedHat
  vars: 
    package_name: '{{ slurm_server_packages }}'
    disable_gpg_check_var: True
  include_role: 
    name: safe_yum
  when: ansible_os_family == 'RedHat'
  
- name: Render systemd units for slurm and slurmdbd
  become: true
  template:
    src: 'systemd/{{ item }}.service.j2'
    dest: '/lib/systemd/system/{{ item }}.service'
    backup: "yes"
  with_items:
    - slurmdbd
    - slurmctld

- name: Create systemd unit dirs
  become: true
  file:
    name: '/etc/systemd/system/{{ item }}.service.d'
    state: directory
  with_items:
    - munge
    - slurmdbd
    - slurmctld

- name: Render systemd units for slurm, slurmdbd and munge
  become: true
  template:
    src: 'systemd/{{ item }}.service.d/unit.conf.j2'
    dest: '/etc/systemd/system/{{ item }}.service.d/unit.conf'
    backup: "yes"
  with_items:
    - munge
    - slurmdbd
    - slurmctld

- block:
    - name: Create {{ slurm_nfs_path }}/spool/slurm
      become: true
      file:
        name: "{{ slurm_nfs_path }}/spool/slurm"
        state: directory
        owner: 'slurm'
        group: 'slurm'
        mode: '0750'
        recurse: yes

    - name: Create munge.key
      become: true
      command: 'dd if=/dev/urandom bs=1 count=1024 of={{ munge_conf_path }}/munge.key'
      args:
        creates: '{{ munge_conf_path }}/munge.key'

    - name: Set munge.key permissions
      become: true
      file:
        name: '{{ munge_conf_path }}/munge.key'
        state: file
        owner: munge
        group: munge
        mode: 0400
      notify: restart munge

    - name: copy munge.key
      become: true
      shell:
        cmd: cp /etc/munge/munge.key /tmp/munge.key
        warn: false
    - name: set permissions
      become: true
      shell:
        cmd: chown {{ ansible_user }}:{{ ansible_user }} /tmp/munge.key
        warn: false
    
- name: Create DB for accounting
  become: true
  mysql_db:
      config_file: '/root/.my.cnf'
      name: '{{ slurm_db_name }}'
      state: present

- name: Create DB user for accounting
  become: true
  mysql_user:
      config_file: '/root/.my.cnf'
      name: '{{ slurm_db_user }}'
      password: '{{ slurmdbd_sql_pwd }}'
      priv: '{{ slurm_db_name }}.*:ALL'
      state: present

- name: Copy cgroup file
  become: true
  copy:
    src: '{{ cgroup_conf_file }}'
    dest: '{{ slurm_conf_path }}/cgroup.conf'
    force: no
    owner: slurm
    group: slurm

- name: Generate slurmdbd.conf
  become: true
  template: 
    src: '{{ slurmdbd_conf_file }}'
    dest: '{{ slurm_conf_path }}/slurmdbd.conf'
    mode: '0600'
    owner: slurm
    group: slurm
    backup: yes
  
- name: Generate slurm.conf
  become: true
  template: 
    src: '{{ slurm_conf_file }}'
    dest: '{{ slurm_conf_path }}/slurm.conf'
    mode: '0644'
    backup: yes
    owner: slurm
    group: slurm
  notify: restart slurm server
  when: initial | bool

- name: add alias for node status
  lineinfile:
    path: '/home/{{ ansible_user }}/.bashrc'
    line: alias status='tail /opt/oci-hpc/logs/crontab_slurm.log | grep -A50 -m1 -e `date +"%Y-%m-%d"`'
    state: present

- name: add alias for max nodes distributed evenly
  lineinfile:
    path: '/home/{{ ansible_user }}/.bashrc'
    line: alias max_nodes="python3 /opt/oci-hpc/bin/max_nodes_partition.py"
    state: present

- name: add alias for validation of number of nodes, pcie, and gpu throttle check
  lineinfile:
    path: '/home/{{ ansible_user }}/.bashrc'
    line: alias validate="python3 /opt/oci-hpc/bin/validation.py"
    state: present

- name: Generate gres.conf
  become: true
  template:
    src: gres.conf.j2
    dest: '{{ slurm_conf_path }}/gres.conf'
    mode: '0644'
    backup: yes
    
- name: Generate topology.conf
  become: true
  template: 
    src: topology.conf.j2
    dest: '{{ slurm_conf_path }}/topology.conf'
    mode: '0644'
    force: no
    owner: slurm
    group: slurm
  delegate_to: 127.0.0.1
  register: initial_topology
  run_once: true
  notify: reconfigure slurm


- name: run handlers
  meta: flush_handlers

- name: Generate a list of types to check
  vars:
    temp_list: "{% for partition in queues %}{% for instancetype in partition.instance_types %}{{partition.name}}-{{instancetype.instance_keyword}},{% endfor %}{% endfor %}"
  set_fact:
    nodesname_list: "{{temp_list.split(',')[:-1] }}"
  when: not initial_topology.changed

- name: Check if shapes need to be added
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName=inactive-{{item}}\\sNodes.*"
    state: absent
  check_mode: yes
  with_items: "{{nodesname_list}}"
  run_once: true
  delegate_to: 127.0.0.1
  register: shape_added
  when: not initial_topology.changed

- name: Add new shapes to existing topology.conf
  become: true
  vars:
    size: "{{ hostvars[inventory_hostname]['private_subnet'] | ipaddr('size') }}"
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName=inactive-{{item.item}}\\sNodes.*"
    line: "SwitchName=inactive-{{item.item}} Nodes={{item.item}}-node-[1-{{size}}]"
    state: present
  with_items: "{{shape_added.results}}"
  run_once: true
  delegate_to: 127.0.0.1
  when: not initial_topology.changed and not ( item.changed | bool)
  notify: reconfigure slurm