---
- name: change Node Status
  become: true
  command: "scontrol update nodename={{ ansible_hostname }} state=future reason=terminating"
  ignore_errors: true
  ignore_unreachable: True
  delegate_to: 127.0.0.1

- name: Remove to the gres.conf file on the controller.
  become: true
  blockinfile:
    path: "{{ slurm_conf_path }}/gres.conf"
    marker: "### {mark} {{ansible_hostname}}"
  delegate_to: 127.0.0.1

- name: Get hostnames
  set_fact:
    nodes_to_remove_temp: "{{hostvars[item]['ansible_hostname']}}"
  with_items: "{{ play_hosts | difference(groups['controller']) | difference(groups['slurm_backup'])  | difference(groups['login']) | difference(groups['monitoring'])}}"
  run_once: true
  register: nodes_to_remove_temp_results

- name: Make a list
  set_fact: nodes_to_remove="{{nodes_to_remove_temp_results.results | map(attribute='ansible_facts.nodes_to_remove_temp') | list}}"
  run_once: true
  
- name: Run the script to get the RackID
  shell: 'curl -sH "Authorization: Bearer Oracle" -L http://169.254.169.254/opc/v1/host | jq .rackId'
#  shell: echo $RANDOM | md5sum | head -c 20
  register: rackID_script
  retries: 5
  delay: 5
  until: rackID_script is not failed

- name: H100 Block
  block: 
    - name: Run the script to get the RDMA Block ID
      shell: 'curl -sH "Authorization: Bearer Oracle" -L http://169.254.169.254/opc/v1/host/rdmaTopologyData/customerLocalBlock | grep ocid'
    #  shell: echo $RANDOM | md5sum | head -c 20
      register: blockID_script
      retries: 3
      delay: 1
      ignore_errors: true
      until: blockID_script is not failed

    - name: Set BlockID fact
      set_fact:
        rackID: "{{ blockID_script.stdout.split('.')[4][-16:-1]}}"
      when: blockID_script is not failed 

    - name: Set RackID fact
      set_fact:
        rackID: "{{ rackID_script.stdout[1:-41]}}"
      when: blockID_script is failed
  when: shape == 'BM.GPU.H100.8' or  shape == 'BM.GPU.H200.8'

- name: Set Rack ID fact on nodes other than H100
  set_fact:
    rackID: "{{ rackID_script.stdout[1:-41]}}"
  when: shape != 'BM.GPU.H100.8' and shape != 'BM.GPU.H200.8'

- name: Get rackIDs
  set_fact:
    racks_to_remove_temp: "{{cluster_name}}:{{hostvars[item]['rackID']}}"
  with_items: "{{ play_hosts | difference(groups['controller']) | difference(groups['slurm_backup'])  | difference(groups['login']) | difference(groups['monitoring'])}}"
  run_once: true
  register: racks_to_remove_temp_results

- name: Make a list of racks to remove
  set_fact: racks_to_remove="{{racks_to_remove_temp_results.results | map(attribute='ansible_facts.racks_to_remove_temp') | list | unique}}"
  run_once: true

- name: Get nodes in Switch
  shell: "scontrol show hostname `cat {{ slurm_conf_path }}/topology.conf | grep \"{{item}}\" | grep Nodes | awk '{ print $2}' | cut -c 7- | tr '\n' ',' | sed 's/,$/\\n/'`"
  register: nodes_in_switch
  delegate_to: 127.0.0.1
  with_items: "{{racks_to_remove}}"

- name: Remove Rack Completely
  debug:
    msg: "Removing rack {{item.item}}"
  run_once: true
  delegate_to: 127.0.0.1
  with_items: "{{nodes_in_switch.results}}"
  when: (item.stdout_lines | difference(nodes_to_remove) | length ) == 0

- name: Remove Node From Rack
  debug:
    msg: "In Rack {{item.item}}, leave only nodes {{item.stdout_lines | difference(nodes_to_remove) }}"
  run_once: true
  delegate_to: 127.0.0.1
  with_items: "{{nodes_in_switch.results}}"
  when: (item.stdout_lines | difference(nodes_to_remove) | length ) > 0

- name: Remove Rack Completely
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{item.item}}\\sNodes.*"
    state: absent
  run_once: true
  delegate_to: 127.0.0.1
  with_items: "{{nodes_in_switch.results}}"
  when: (item.stdout_lines | difference(nodes_to_remove) | length ) == 0

- name: Remove Nodes from Rack Switch
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{item.item}}\\sNodes.*"
    line: "SwitchName={{item.item}} Nodes={{ item.stdout_lines | difference(nodes_to_remove) | join(',') }}"
    state: present
  run_once: true
  delegate_to: 127.0.0.1
  with_items: "{{nodes_in_switch.results}}"
  when: (item.stdout_lines | difference(nodes_to_remove) | length ) > 0 and (item.stdout != "" ) 

- name: Get left Over racks from Cluster
  block:
    - name: Get Racks left from topology.conf
      shell: "cat {{ slurm_conf_path }}/topology.conf | grep \"SwitchName={{cluster_name}}:\" | grep \" Nodes=\" | awk '{print $1}' | sed 's/SwitchName=//' | tr '\n' ',' | sed 's/,$/\\n/'"
      register: racks_left
      run_once: true
      delegate_to: 127.0.0.1
    - name: Create list of racks
      set_fact:
        racks_left_list: "{% if racks_left.stdout != '' %}{{racks_left.stdout.split(',') | list}}{% else %}[]{% endif %}"
      run_once: true
  rescue:
    - name: Empty racks
      set_fact:
        racks_left_list: []
      run_once: true

- name: Remove Cluster Completely
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{cluster_name}}\\sSwitches=.*"
    state: absent
  run_once: true
  delegate_to: 127.0.0.1
  when: racks_left_list | list | length == 0

- name: Update Rack switches for the cluster
  become: true
  lineinfile:
    path: "{{ slurm_conf_path }}/topology.conf"
    regexp: "SwitchName={{cluster_name}}\\sSwitches.*"
    line: "SwitchName={{cluster_name}} Switches={{ racks_left_list | join(',') }}"
    state: present
  run_once: true
  delegate_to: 127.0.0.1
  when: racks_left_list | list | length > 0

- name: Reconfigure Slurm for topology
  become: true
  command: "scontrol reconfigure"
  delegate_to: 127.0.0.1
  run_once: true
  ignore_errors: yes