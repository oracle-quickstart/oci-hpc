#!/bin/bash
#SBATCH --job-name=nccl-allreduce-slurm
#SBATCH --nodes=32
#SBATCH --gpus-per-node=8
#SBATCH --ntasks-per-node=8
#SBATCH --exclusive
export PMI_DEBUG=1


cd /nfs/scratch
mkdir $SLURM_JOB_ID
cd $SLURM_JOB_ID


MACHINEFILE="hostfile"
ORDEREDMACHINEFILE="ordered_hostfile_system_name"

scontrol show hostnames $SLURM_JOB_NODELIST > $MACHINEFILE
cat $MACHINEFILE

python3 /home/opc/node_ordering_by_rack.py --input_file $MACHINEFILE > /dev/null
# openmpi
#sed -i "s/$/ slots=${SLURM_NTASKS_PER_NODE}/" $ORDEREDMACHINEFILE


echo ORDEREDMACHINEFILE
cat $ORDEREDMACHINEFILE
echo ORDEREDMACHINEFILE

#y="" ; for x in `less $ORDEREDMACHINEFILE` ; do  y="$y$x:8," ; echo $y ; host_list=`echo $y | sed  's|,$||g'` ;   done ;  echo $host_list
y="" ; for x in `less $ORDEREDMACHINEFILE` ; do  y="$y$x," ; echo $y ; host_list=`echo $y | sed  's|,$||g'` ;   done ;  echo $host_list

if [ -f /usr/mpi/gcc/openmpi-4.1.0rc5/bin/mpivars.sh ]; then
  source /usr/mpi/gcc/openmpi-4.1.0rc5/bin/mpivars.sh
else
  source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh
fi

#source /usr/mpi/gcc/openmpi-4.1.0rc5/bin/mpivars.sh
#source /usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpivars.sh

export NCCL_DEBUG=WARN


#mpirun -d --mca pml ucx -x SLURM_JOB_NODELIST=$host_list --bind-to numa  -x NCCL_DEBUG=WARN  -x NCCL_IB_SL=0 -x NCCL_IB_TC=41 -x NCCL_IB_QPS_PER_CONNECTION=4 -x NCCL_IB_GID_INDEX=3 -x NCCL_ALGO=Ring -x NCCL_TOPO_FILE=/home/opc/topo-flattened-b4.xml -x NCCL_IB_HCA="mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_11,mlx5_12,mlx5_13,mlx5_16,mlx5_17,mlx5_18,mlx5_19"   -x UCX_NET_DEVICES=mlx5_0:1 -x HCOLL_ENABLE_MCAST_ALL=0 -x coll_hcoll_enable=0 -x UCX_TLS=ud,self,sm  -np $((SLURM_NNODES*SLURM_NTASKS_PER_NODE)) --rankfile rankfile_system_name  /home/opc/nccl-tests/build/all_reduce_perf -b1G -e10G -i$((1024*1024*1024*9))  -n 100
# no need to pass:  -x SLURM_JOB_NODELIST=$host_list

  mpirun --mca pml ucx \
  --bind-to numa \
  -x NCCL_DEBUG=WARN \
  -x NCCL_IB_SL=0 \
  -x NCCL_IB_TC=41 \
  -x NCCL_IB_QPS_PER_CONNECTION=4 \
  -x UCX_TLS=ud,self,sm \
{% if shape == "BM.GPU.B4.8" %}
  -x UCX_NET_DEVICES=mlx5_0:1 \
{% elif shape == "BM.GPU4.8" %}
  -x UCX_NET_DEVICES=mlx5_4:1 \
{% endif %}
  -x HCOLL_ENABLE_MCAST_ALL=0 \
  -x coll_hcoll_enable=0 \
  -x NCCL_IB_GID_INDEX=3 \
  -x NCCL_ALGO=Ring \
{% if shape == "BM.GPU.B4.8" %}
  -x NCCL_TOPO_FILE=/home/opc/topo-flattened-b4.xml \
{% elif shape == "BM.GPU4.8" %}
  -x NCCL_TOPO_FILE=/home/opc/topo-flattened.xml \
{% endif %}
{% if shape == "BM.GPU.B4.8" %}
  -x NCCL_IB_HCA="mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_11,mlx5_12,mlx5_13,mlx5_16,mlx5_17,mlx5_18,mlx5_19" \
{% elif shape == "BM.GPU4.8" %}
  -x NCCL_IB_HCA="mlx5_0,mlx5_2,mlx5_6,mlx5_8,mlx5_10,mlx5_12,mlx5_14,mlx5_16,mlx5_1,mlx5_3,mlx5_7,mlx5_9,mlx5_11,mlx5_13,mlx5_15,mlx5_17" \
{% endif %}
  --np $((SLURM_NNODES*SLURM_NTASKS_PER_NODE))  --rankfile rankfile_system_name  /home/opc/nccl-tests/build/all_reduce_perf -b1G -e10G -i$((1024*1024*1024*9)) -n 100


